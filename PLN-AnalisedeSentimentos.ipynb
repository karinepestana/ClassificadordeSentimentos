{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/karine/anaconda3/lib/python3.7/site-packages (3.4.5)\r\n",
      "Requirement already satisfied: six in /home/karine/anaconda3/lib/python3.7/site-packages (from nltk) (1.14.0)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to /home/karine/nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/karine/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/karine/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Preparando ambiente (importando bibliotecas e downloads...)\n",
    "\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('rslp')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import spacy\n",
    "nlp = spacy.load('pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregamento e tratamento dos léxicos e datasets necessários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicionario_lexico = {}\n",
    "sentilexpt = open('lexicos/SentiLex-lem-PT02.txt','r', encoding='utf8')\n",
    "ontolp = open('lexicos/lexico_v3.0.txt','r', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sentilexpt:\n",
    "    pos_ponto = i.find('.')\n",
    "    palavra = (i[:pos_ponto])\n",
    "    pol_pos = i.find('POL')\n",
    "    polaridade = (i[pol_pos+7:pol_pos+9]).replace(';','')\n",
    "    dicionario_lexico[palavra] = polaridade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ontolp:\n",
    "    split_dic = i.split(',')\n",
    "    palavra = split_dic[0]\n",
    "    if palavra not in dicionario_lexico:\n",
    "        polaridade = split_dic[2]\n",
    "        dicionario_lexico[palavra] = polaridade        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_sentimentos = {}\n",
    "sentidic = open('dic_sentimento_3_niveis_balanceado.txt','r', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sentidic:\n",
    "    split_dic = i.split('\\t')\n",
    "    dic_sentimentos[split_dic[0]] = split_dic[1][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_words = open('lexicos/degree-words.txt','r', encoding='utf8')\n",
    "degree_words_set = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in degree_words:\n",
    "    split_dic = i.split(';')\n",
    "    degree_words_set[split_dic[0]] = split_dic[1][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentilexpt.close()\n",
    "ontolp.close()\n",
    "degree_words.close()\n",
    "sentidic.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc = pd.read_csv(\"lexicos/LIWC2007.txt\",sep=\"\\t\",encoding=\"ISO-8859-1\", names=['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_set = {}\n",
    "def cria_dic_liwc(x):\n",
    "    lista = []\n",
    "    if 126 in list(x):\n",
    "        liwc_set[x.name] = 'pos'\n",
    "    elif 127 in list(x):\n",
    "        liwc_set[x.name] = 'neg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a          None\n",
       "aba        None\n",
       "abafa      None\n",
       "abafad*    None\n",
       "abafada    None\n",
       "           ... \n",
       "último     None\n",
       "último*    None\n",
       "út*        None\n",
       "úteis      None\n",
       "útil       None\n",
       "Length: 127161, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liwc.apply(lambda x: cria_dic_liwc(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taxa_sent_por_frase(frase):\n",
    "    \n",
    "    frase = nlp(frase)\n",
    "    degree_words_head = []\n",
    "    degree_words_aux = []\n",
    "    score_frase = {'alegria':0, 'tristeza':0, 'raiva':0, 'medo':0, 'nojo':0}\n",
    "    \n",
    "    for token in frase:\n",
    "        if(token.text in degree_words_set):\n",
    "            degree_words_head.append(token.head.text)\n",
    "            degree_words_aux.append(int(degree_words_set.get(token.text)))\n",
    "            \n",
    "    for token in frase:\n",
    "        if(token.text in dic_sentimentos):\n",
    "            dic = dic_sentimentos.get(token.text)\n",
    "            neg = 0\n",
    "            if token.head.text in degree_words_head:                \n",
    "                neg = int(degree_words_aux[degree_words_head.index(token.head.text)])\n",
    "            score_frase[dic] = int(score_frase.get(dic)) + 1 + neg\n",
    "\n",
    "    score_resultado = ('neutro', 0)\n",
    "    for sentimento, valor in score_frase.items():\n",
    "        s, v = score_resultado\n",
    "        if abs(valor) > v:\n",
    "            score_resultado = (sentimento, valor)\n",
    "        \n",
    "    s, v = score_resultado\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento do Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords():\n",
    "    frases = []\n",
    "    for (palavras, sentimento) in zip(logs.Frase, logs.Sentimento):\n",
    "        semStop = [ p for p in palavras.split() if p not in stop_words]\n",
    "        frases.append((semStop, sentimento))\n",
    "    return frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def realiza_stem(dados):\n",
    "    stemmer = nltk.stem.RSLPStemmer()\n",
    "    frases_sem_Stemming = []\n",
    "    for (palavras, sentimento) in dados:\n",
    "        com_Stemming = [str(stemmer.stem(p)) for p in palavras.split() if p not in stop_words]\n",
    "        frases_sem_Stemming.append((com_Stemming, sentimento))\n",
    "    return pd.DataFrame(frases_sem_Stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessamento():\n",
    "    sem_stopwords = removeStopWords()    \n",
    "    return realiza_stem(sem_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e21b00952d1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtext_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logs' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer(ngram_range=(1, 1))\n",
    "vect.fit(logs.Frase)\n",
    "text_vect = vect.transform(logs.Frase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(text_vect, logs.Sentimento, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0, solver='newton-cg')\n",
    "#y=y.astype('int')\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "y_prediction = clf.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test, average='weighted')\n",
    "\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentimentos por transação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentos = ['alegria','medo','nojo','raiva','tristeza']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentimento_transacao(transacao):\n",
    "    lista_sentimentos = []\n",
    "    sent_max = ''\n",
    "    sent_v = 0\n",
    "    for frase in transacao:\n",
    "        res = frase.split('\\t')\n",
    "        lista_sentimentos.append(taxa_sent_por_frase(res[1]))        \n",
    "    for item in sentimentos:\n",
    "        v_sent_atual = lista_sentimentos.count(item)\n",
    "        if v_sent_atual > sent_v:\n",
    "            sent_v = v_sent_atual\n",
    "            sent_max = item\n",
    "    return sent_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alegria\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "alegria\n",
      "alegria\n",
      "medo\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "alegria\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "nojo\n",
      "raiva\n",
      "alegria\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "alegria\n",
      "raiva\n",
      "tristeza\n",
      "tristeza\n",
      "tristeza\n",
      "medo\n",
      "tristeza\n",
      "alegria\n",
      "medo\n",
      "raiva\n",
      "medo\n",
      "alegria\n",
      "medo\n",
      "\n",
      "nojo\n",
      "medo\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "alegria\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "medo\n",
      "alegria\n",
      "alegria\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "medo\n",
      "raiva\n",
      "alegria\n",
      "tristeza\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "medo\n",
      "raiva\n",
      "medo\n",
      "alegria\n",
      "alegria\n",
      "raiva\n",
      "raiva\n",
      "alegria\n",
      "alegria\n",
      "raiva\n",
      "alegria\n",
      "medo\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "\n",
      "alegria\n",
      "alegria\n",
      "medo\n",
      "medo\n",
      "alegria\n",
      "\n",
      "nojo\n",
      "alegria\n",
      "nojo\n",
      "\n",
      "alegria\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "alegria\n",
      "alegria\n",
      "alegria\n",
      "alegria\n",
      "alegria\n",
      "medo\n",
      "tristeza\n",
      "tristeza\n",
      "alegria\n",
      "tristeza\n",
      "medo\n",
      "medo\n",
      "medo\n",
      "medo\n",
      "medo\n",
      "nojo\n",
      "alegria\n",
      "raiva\n",
      "alegria\n",
      "alegria\n",
      "alegria\n",
      "raiva\n",
      "alegria\n",
      "raiva\n",
      "raiva\n",
      "alegria\n",
      "raiva\n",
      "medo\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "raiva\n",
      "raiva\n"
     ]
    }
   ],
   "source": [
    "registros = open('logs/Logs_transacoes/Registros.txt', 'r', encoding='utf8')\n",
    "for i in registros:\n",
    "    aux = str('logs/Logs_transacoes/Corrigidos/' + i[:-1] + \".txt\")\n",
    "    log_transacao = open(aux, 'r', encoding='utf8')\n",
    "    #sentimento_transacao(log_transacao)\n",
    "    print(sentimento_transacao(log_transacao))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "NPL.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
